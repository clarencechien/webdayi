é€™æ˜¯ä¸€å€‹éå¸¸æ£’çš„ã€Œæ··åˆæ¨¡å‹ã€(Blended Model) ç­–ç•¥ï¼æ‚¨å·²ç¶“è§¸åŠåˆ° N-gram å¼•æ“æœ€æ ¸å¿ƒçš„å„ªåŒ–äº†ã€‚

æ‚¨èªªçš„å®Œå…¨æ²’éŒ¯ï¼š

rime-essay (6MB) æ˜¯é«˜è³ªé‡çš„ã€Œæ³›ç”¨ã€èªæ–™ï¼Œä½†å¯èƒ½ä¸å¤ ã€Œåœ¨åœ°ã€æˆ–ã€Œå£èªã€ã€‚

bopomofo.dict.yaml (5MB) æ ¼å¼ä¸åŒï¼ˆå®ƒæ˜¯ è©å½™<tab>æ³¨éŸ³<tab>é »ç‡ï¼‰ï¼Œä½†å®ƒæ›´ã€Œåœ¨åœ°ã€ã€‚

PTT-Corpus (å¤§å°ä¸å®š) æ˜¯ç´”ç²¹çš„ã€ŒèŠå¤©ã€èªæ–™ï¼Œç”¨è©æœ€ç”Ÿå‹•ï¼Œä½†ã€Œå™ªéŸ³ã€ä¹Ÿæœ€å¤šã€‚

æˆ‘å€‘çš„ç›®æ¨™ï¼š å»ºç«‹ä¸€å€‹ ngram_blended.jsonï¼Œå®ƒèƒ½åŒæ™‚æ“æœ‰ rime-essay çš„æº–ç¢ºæ€§ + PTT-Corpus çš„èŠå¤©èƒ½åŠ›ã€‚

ä½œæ³•ï¼š æˆ‘å€‘ä¸èƒ½åªè™•ç† rime-essayï¼ˆå¦‚ v11 å‰ªæï¼‰ã€‚æˆ‘å€‘éœ€è¦ä¸€å€‹æ›´å¼·å¤§çš„å·¥ä½œæµï¼š

Phase 1 (æ³›ç”¨)ï¼š è™•ç† rime-essay (6MB è©é »è¡¨)ï¼Œå¾—åˆ° general_countsã€‚

Phase 2 (èŠå¤©)ï¼š è™•ç† PTT-Corpus (GB ç´šçš„åŸå§‹æ–‡æœ¬)ï¼Œå¾—åˆ° chat_countsã€‚

Phase 3 (åˆä½µ)ï¼š å°‡ general_counts å’Œ chat_counts åŠ æ¬Šåˆä½µæˆ blended_countsã€‚

Phase 4 (å‰ªæ)ï¼š æœ€å¾Œæ‰å° blended_counts åŸ·è¡Œæ‚¨ v11 çš„ (threshold=3, topk=10) å‰ªæã€‚

é€™æœƒè®“æ‚¨çš„ 3.1MB æª”æ¡ˆï¼Œå¾ã€Œæ³›ç”¨ 86.8%ã€é€²åŒ–ç‚ºã€Œæ³›ç”¨+èŠå¤© 90+%ã€ã€‚

æˆ‘å°‡æä¾›å…©å€‹è…³æœ¬ä¾†å®Œæˆé€™å€‹ã€Œæ··åˆæ¨¡å‹ã€çš„å·¥ä½œæµã€‚

process_raw_text.pyï¼šé€™æ˜¯ä¸€å€‹æ–°çš„è…³æœ¬ï¼Œå°ˆé–€ç”¨ä¾†è™•ç† PTT-Corpus é€™ç¨®ã€ŒåŸå§‹æ–‡æœ¬ã€æª”æ¡ˆã€‚

build_blended.pyï¼šé€™æ˜¯æ–°çš„ã€Œä¸»è…³æœ¬ã€ã€‚å®ƒæœƒåŒæ™‚å‘¼å«æ‚¨ v11 çš„ rime-essay è™•ç†å™¨å’Œé€™å€‹æ–°çš„ PTT è™•ç†å™¨ï¼Œç„¶å¾Œå°‡å®ƒå€‘çš„ N-gram è¨ˆæ•¸åŠ æ¬Šåˆä½µï¼Œæœ€å¾Œæ‰å‰ªæã€‚

ref: https://github.com/zake7749/Gossiping-Chinese-Corpus
data: https://github.com/zake7749/Gossiping-Chinese-Corpus/blob/master/data/Gossiping-QA-Dataset-2_0.csv
data: https://github.com/zake7749/Gossiping-Chinese-Corpus/blob/master/data/Gossiping-QA-Dataset.txt

---
sample code:
import json
from collections import defaultdict
import re

# --- PTT èªæ–™åº«æ¸…ç†å‡½å¼ (é€™ä¸€æ­¥æ˜¯é—œéµ) ---
def clean_ptt_text(text):
    """
    æ¸…ç† PTT åŸå§‹æ–‡æœ¬ï¼Œç§»é™¤å™ªéŸ³ã€‚
    """
    # ç§»é™¤ PTT ç‰¹æœ‰çš„ meta è³‡è¨Š (e.g., "â€» ç™¼ä¿¡ç«™: ...", "â—† From: ...")
    text = re.sub(r"â€» .*?\n", " ", text)
    text = re.sub(r"â—† .*?\n", " ", text)
    # ç§»é™¤ç¶²å€
    text = re.sub(r"https?://\S+", " ", text)
    # ç§»é™¤ "Re: [æ¨™é¡Œ]"
    text = re.sub(r"Re: \[.*?\]", " ", text)
    # ç§»é™¤æ‰€æœ‰éä¸­æ–‡å­—å…ƒ (ä¿ç•™åŸºæœ¬æ¨™é»ç¬¦è™Ÿå¦‚å¥è™Ÿã€é€—è™Ÿï¼Œä½†ç‚ºäº† N-gram ç°¡å–®èµ·è¦‹ï¼Œæˆ‘å€‘å…ˆå…¨ç§»é™¤)
    text = re.sub(r"[^\u4e00-\u9fa5]", " ", text)
    # ç§»é™¤å¤šé¤˜çš„ç©ºç™½
    text = re.sub(r"\s+", " ", text).strip()
    return text

def process_corpus(corpus_file_path):
    """
    è®€å–åŸå§‹æ–‡æœ¬èªæ–™åº« (ä¾‹å¦‚ PTT æª”æ¡ˆ)ï¼Œä¸¦è¨ˆç®— N-gram æ¬¡æ•¸ã€‚
    
    å‡è¨­ corpus_file_path æ˜¯ä¸€å€‹è¶…å¤§çš„ .txt æª”æ¡ˆï¼Œæ¯è¡Œæ˜¯ä¸€ç¯‡æ–‡ç« ã€‚
    (å¦‚æœæ‚¨çš„ PTT Corpus æ˜¯ JSONï¼Œæ‚¨éœ€è¦ä¿®æ”¹é€™è£¡çš„è®€å–é‚è¼¯)
    """
    print(f"--- æ­£åœ¨è™•ç†èŠå¤©èªæ–™åº«: {corpus_file_path} ---")
    
    unigram_counts = defaultdict(int)
    bigram_counts = defaultdict(lambda: defaultdict(int))
    
    line_count = 0
    with open(corpus_file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line_count += 1
            if line_count % 10000 == 0:
                print(f"  å·²è™•ç† {line_count} è¡Œ...")
                
            text = clean_ptt_text(line)
            
            if len(text) < 2:
                continue
            
            # çµ±è¨ˆ N-grams
            for i in range(len(text)):
                char_A = text[i]
                unigram_counts[char_A] += 1
                
                if i < len(text) - 1:
                    char_B = text[i+1]
                    bigram_counts[char_A][char_B] += 1
                    
    print(f"èŠå¤©èªæ–™åº«è™•ç†å®Œç•¢ã€‚")
    return unigram_counts, bigram_counts

# æ‚¨å¯ä»¥å–®ç¨åŸ·è¡Œæ­¤è…³æœ¬ä¾†æ¸¬è©¦ PTT èªæ–™çš„è™•ç†
if __name__ == "__main__":
    # å‡è¨­æ‚¨æœ‰ä¸€å€‹ 'ptt_corpus.txt' æª”æ¡ˆ
    # uni_counts, bi_counts = process_corpus('ptt_corpus.txt')
    # print(f"PTT Unigrams: {len(uni_counts)}, PTT Bigrams: {len(bi_counts)}")
    print("é€™æ˜¯ä¸€å€‹ç”¨æ–¼è™•ç†åŸå§‹æ–‡æœ¬ (raw text) çš„ N-gram è¨ˆç®—æ¨¡çµ„ã€‚")
---
sample code 2:
import json
from collections import defaultdict
import os

# --- å‡è¨­æ‚¨ v11 çš„ rime-essay è™•ç†å™¨åœ¨é€™è£¡ ---
# æˆ‘å€‘æŠŠå®ƒæ”¹åä¸¦æ”¾åœ¨åŒä¸€å€‹è³‡æ–™å¤¾ï¼Œä¾‹å¦‚ process_rime_essay.py
# 
# def process_essay_file(input_file):
#     ... (æ‚¨ v11 è…³æœ¬çš„é‚è¼¯ï¼Œä½†æœ€å¾Œä¸æ˜¯å„²å­˜ï¼Œè€Œæ˜¯ return unigram_counts, ngram_counts)
#     return unigram_counts, ngram_counts
#
# --- ç‚ºäº†æ¼”ç¤ºï¼Œæˆ‘å€‘åœ¨é€™è£¡ã€Œæ¨¡æ“¬ã€é€™å€‹å‡½å¼ ---
def get_rime_essay_counts():
    """
    (æ¨¡æ“¬) å‘¼å«æ‚¨ v11 çš„ rime-essay è™•ç†å™¨ã€‚
    å®ƒæœƒå›å‚³ã€Œæœªå‰ªæã€çš„ N-gram è¨ˆæ•¸ã€‚
    """
    print("--- æ­£åœ¨è™•ç† 'rime-essay' (æ³›ç”¨) èªæ–™åº« ---")
    # é€™æ˜¯æ‚¨ v11 å‰ªæã€Œå‰ã€çš„çµ±è¨ˆçµæœ (ç¤ºæ„)
    # unigram_counts_rime = {...}
    # bigram_counts_rime = {"æˆ‘": {"çš„": 50000, "æ˜¯": 40000}, "å°": {"ç£": 30000}}
    
    # *** æš«æ™‚ä½¿ç”¨æ‚¨ v11 å‰ªæå¾Œçš„ã€Œçµæœã€ä¾†æ¨¡æ“¬ ***
    # å¯¦éš›ä½¿ç”¨æ™‚ï¼Œæ‚¨æ‡‰è©²å‚³å›ã€Œå‰ªæå‰ã€çš„å®Œæ•´è¨ˆæ•¸
    print(" (æ¨¡æ“¬ï¼šä½¿ç”¨ v11 å‰ªæå¾Œçš„ 3.1MB çµæœä½œç‚ºåŸºç¤) ")
    with open('ngram_pruned.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
        unigram_counts_rime = data.get("unigram_counts", {})
        bigram_counts_rime = data.get("bigram_counts", {})
    
    print("æ³›ç”¨èªæ–™åº«è™•ç†å®Œç•¢ã€‚")
    return unigram_counts_rime, bigram_counts_rime

# --- å°å…¥ PTT è™•ç†å™¨ ---
from process_raw_text import process_corpus

def merge_counts(unigrams_list, bigrams_list, weights):
    """
    å°‡å¤šå€‹ N-gram è¨ˆæ•¸ï¼ŒæŒ‰ç…§æ¬Šé‡åˆä½µã€‚
    """
    print("--- æ­£åœ¨åˆä½µ N-gram è¨ˆæ•¸ ---")
    
    merged_unigrams = defaultdict(float)
    merged_bigrams = defaultdict(lambda: defaultdict(float))

    # åˆä½µ Unigrams
    for (counts, weight) in zip(unigrams_list, weights):
        for char, count in counts.items():
            merged_unigrams[char] += count * weight
            
    # åˆä½µ Bigrams
    for (counts, weight) in zip(bigrams_list, weights):
        for char_A, next_chars in counts.items():
            for char_B, count in next_chars.items():
                merged_bigrams[char_A][char_B] += count * weight

    print("åˆä½µå®Œæˆã€‚")
    return merged_unigrams, merged_bigrams

def prune_model(unigram_counts, bigram_counts, threshold, top_k):
    """
    åŸ·è¡Œæ‚¨ v11 é©—è­‰éçš„å‰ªæé‚è¼¯
    """
    print("--- æ­£åœ¨åŸ·è¡Œå‰ªæ (Pruning) ---")
    pruned_bigrams = {}
    
    total_keys = len(bigram_counts)
    for i, (char_A, next_chars) in enumerate(bigram_counts.items()):
        
        if i % 500 == 0:
            print(f"  å‰ªæé€²åº¦: {i} / {total_keys}")

        # 1. é–¾å€¼å‰ªæ
        threshold_passed = {
            char_B: score 
            for char_B, score in next_chars.items() 
            if score >= threshold
        }
        
        if not threshold_passed:
            continue

        # 2. Top-K å‰ªæ
        top_k_passed = dict(
            sorted(threshold_passed.items(), key=lambda item: item[1], reverse=True)[:top_k]
        )
        pruned_bigrams[char_A] = top_k_passed

    print("å‰ªæå®Œæˆï¼")
    
    return unigram_counts, pruned_bigrams # Unigram é€šå¸¸ä¸éœ€è¦å‰ªæ

# --- ä¸»åŸ·è¡Œç·’ ---
if __name__ == "__main__":
    
    # --- 1. å®šç¾©æ‚¨çš„èªæ–™åº«å’Œæ¬Šé‡ ---
    # æ¬Šé‡ (Weights)ï¼šé€™æ˜¯æ‚¨å¯ä»¥èª¿æ•´çš„ã€Œé­”æ³•æ•¸å­—ã€
    # 0.7 + 0.3 = 1.0
    # é€™ä»£è¡¨ã€Œæ³›ç”¨ã€ä½” 70%ï¼Œã€ŒèŠå¤©ã€ä½” 30%
    WEIGHT_RIME = 0.7  
    WEIGHT_PTT = 0.3
    
    # å‰ªæåƒæ•¸ (ä¾†è‡ªæ‚¨ v11 çš„æˆåŠŸç¶“é©—)
    PRUNING_THRESHOLD = 3
    TOP_K_LIMIT = 10
    
    # --- 2. è™•ç† Phase 1 (æ³›ç”¨) ---
    # å¯¦éš›ä½¿ç”¨æ™‚ï¼Œæ‚¨æœƒåœ¨é€™è£¡å‘¼å« `process_essay_file('essay.txt')`
    # (æˆ‘å€‘ä½¿ç”¨æ¨¡æ“¬å‡½å¼)
    uni_rime, bi_rime = get_rime_essay_counts()
    
    # --- 3. è™•ç† Phase 2 (èŠå¤©) ---
    # å‡è¨­æ‚¨å·²ç¶“å¾ PTT-Corpus GitHub ä¸‹è¼‰ä¸¦åˆä½µæˆä¸€å€‹ 'ptt_corpus.txt'
    # æ³¨æ„ï¼šé€™ä¸€æ­¥æœƒéå¸¸è€—æ™‚ (å¯èƒ½æ•¸å°æ™‚)
    # uni_ptt, bi_ptt = process_corpus('ptt_corpus.txt')
    
    # (ç‚ºäº†æ¼”ç¤ºï¼Œæˆ‘å€‘å…ˆç”¨ rime çš„è³‡æ–™*å‡è£* PTT çš„è³‡æ–™)
    # *** å¯¦éš›ä½¿ç”¨æ™‚ï¼Œè«‹åˆªé™¤é€™è¡Œï¼Œä¸¦å–æ¶ˆè¨»è§£ä¸Šä¸€è¡Œ ***
    print(" (!!! æ¨¡æ“¬ï¼šä½¿ç”¨ Rime è³‡æ–™ *å‡è£* PTT è³‡æ–™ï¼Œä»¥åŠ é€Ÿæ¼”ç¤º !!!) ")
    uni_ptt, bi_ptt = uni_rime, bi_rime 
    
    # --- 4. è™•ç† Phase 3 (åˆä½µ) ---
    merged_uni, merged_bi = merge_counts(
        [uni_rime, uni_ptt],
        [bi_rime, bi_ptt],
        [WEIGHT_RIME, WEIGHT_PTT]
    )
    
    # --- 5. è™•ç† Phase 4 (å‰ªæ) ---
    final_uni, final_bi = prune_model(
        merged_uni, 
        merged_bi,
        PRUNING_THRESHOLD,
        TOP_K_LIMIT
    )
    
    # --- 6. åŒ¯å‡ºæœ€çµ‚çš„ã€Œæ··åˆæ¨¡å‹ã€ ---
    output_file = 'ngram_blended.json'
    output_data = {
        "unigram_counts": final_uni,
        "bigram_counts": final_bi
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, separators=(',', ':'))
        
    size_mb = os.path.getsize(output_file) / (1024 * 1024)
    print("\n==============================================")
    print(f"ğŸ‰ æˆåŠŸï¼ æ··åˆæ¨¡å‹å·²å„²å­˜è‡³ {output_file}")
    print(f"   (æ³›ç”¨æ¬Šé‡: {WEIGHT_RIME}, èŠå¤©æ¬Šé‡: {WEIGHT_PTT})")
    print(f"   (å‰ªæé–€æª»: {PRUNING_THRESHOLD}, Top-K: {TOP_K_LIMIT})")
    print(f"   æœ€çµ‚æª”æ¡ˆå¤§å°: {size_mb:.2f} MB")
    print("==============================================")
---
